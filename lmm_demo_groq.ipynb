{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsNSg93Xy6kpx1yglEB9/Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kap6/llm-demo-groq/blob/main/lmm_demo_groq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XIV9UIxPciY7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193550ed-1a25-41da-a700-5ed5410a308c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "langchain\n",
        "langchain-community\n",
        "llama-parse\n",
        "fastembed\n",
        "chromadb\n",
        "python-dotenv\n",
        "langchain-groq\n",
        "chainlit\n",
        "fastembed\n",
        "unstructured[md]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "xE3Qa8PXdIAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n"
      ],
      "metadata": {
        "id": "j_0PXLQBdTBa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p 'content/data/10q/'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10q/uber_10q_march_2022.pdf' -O 'data/10q/uber_10q_march_2022.pdf'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10q/uber_10q_june_2022.pdf' -O 'data/10q/uber_10q_june_2022.pdf'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10q/uber_10q_sept_2022.pdf' -O 'data/10q/uber_10q_sept_2022.pdf'\n",
        "!wget \"https://meetings.wmo.int/Cg-19/PublishingImages/SitePages/FINAC-43/7%20-%20EC-77-Doc%205%20Financial%20Statements%20for%202022%20(FINAC).pptx\" -O './data/presentation.pptx'"
      ],
      "metadata": {
        "id": "0mjW_I7SgcEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### LLAMAPARSE #####\n",
        "from llama_parse import LlamaParse\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "#\n",
        "from groq import Groq\n",
        "from langchain_groq import ChatGroq\n",
        "#\n",
        "import joblib\n",
        "import os\n",
        "import nest_asyncio  # noqa: E402\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "463qrZO4eb5G"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_or_parse_data():\n",
        "    data_file = \"./data/parsed_data.pkl\"\n",
        "\n",
        "    if os.path.exists(data_file):\n",
        "        # Load the parsed data from the file\n",
        "        parsed_data = joblib.load(data_file)\n",
        "    else:\n",
        "        # Perform the parsing step and store the result in llama_parse_documents\n",
        "        parsingInstructionUber10k = \"\"\"The provided document is a quarterly report filed by Uber Technologies,\n",
        "        Inc. with the Securities and Exchange Commission (SEC).\n",
        "        This form provides detailed financial information about the company's performance for a specific quarter.\n",
        "        It includes unaudited financial statements, management discussion and analysis, and other relevant disclosures required by the SEC.\n",
        "        It contains many tables.\n",
        "        Try to be precise while answering the questions\"\"\"\n",
        "        parser = LlamaParse(api_key=llamaparse_api_key,\n",
        "                            result_type=\"markdown\",\n",
        "                            parsing_instruction=parsingInstructionUber10k,\n",
        "                            max_timeout=5000,)\n",
        "        llama_parse_documents = parser.load_data(\"./data/10q/uber_10q_march_2022.pdf\")\n",
        "\n",
        "\n",
        "        # Save the parsed data to a file\n",
        "        print(\"Saving the parse results in .pkl format ..........\")\n",
        "        joblib.dump(llama_parse_documents, data_file)\n",
        "\n",
        "        # Set the parsed data to the variable\n",
        "        parsed_data = llama_parse_documents\n",
        "\n",
        "    return parsed_data"
      ],
      "metadata": {
        "id": "CtQp1F26ehJH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = \"./data/parsed_data.pkl\"\n",
        "parsingInstructionUber10k = \"\"\"The provided document is a quarterly report filed by Uber Technologies,\n",
        "Inc. with the Securities and Exchange Commission (SEC).\n",
        "This form provides detailed financial information about the company's performance for a specific quarter.\n",
        "It includes unaudited financial statements, management discussion and analysis, and other relevant disclosures required by the SEC.\n",
        "It contains many tables.\n",
        "Try to be precise while answering the questions\"\"\"\n",
        "parser = LlamaParse(api_key=llamaparse_api_key,\n",
        "                    result_type=\"markdown\",\n",
        "                    parsing_instruction=parsingInstructionUber10k,\n",
        "                    max_timeout=5000,)\n",
        "llama_parse_documents = parser.load_data(\"./data/10q/uber_10q_march_2022.pdf\")\n",
        "\n",
        "\n",
        "# Save the parsed data to a file\n",
        "print(\"Saving the parse results in .pkl format ..........\")\n",
        "joblib.dump(llama_parse_documents, data_file)\n",
        "\n",
        "# Set the parsed data to the variable\n",
        "parsed_data = llama_parse_documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lObPLxFih3lp",
        "outputId": "222f0815-af0c-4032-bc24-8e3799bf61f7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 3730d2d0-833b-4bd6-a4b4-383bba2e4d71\n",
            "Saving the parse results in .pkl format ..........\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function to either load or parse the data\n",
        "llama_parse_documents = parsed_data\n",
        "print(llama_parse_documents[0].text[:300])\n",
        "\n",
        "with open('data/output.md', 'a') as f:  # Open the file in append mode ('a')\n",
        "  for doc in llama_parse_documents:\n",
        "      f.write(doc.text + '\\n')\n",
        "\n",
        "markdown_path = \"data/output.md\"\n",
        "loader = UnstructuredMarkdownLoader(markdown_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TshJgTHBil5l",
        "outputId": "744bbaf3-9a8f-4d94-d96b-ff74fae58517"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Uber Technologies, Inc. - Form 10-Q\n",
            "\n",
            "# UNITED STATES SECURITIES AND EXCHANGE COMMISSION\n",
            "\n",
            "Washington, D.C. 20549\n",
            "\n",
            "#\n",
            "# FORM 10-Q\n",
            "\n",
            "(Mark One)\n",
            "\n",
            "â˜’ QUARTERLY REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
            "\n",
            "For the quarterly period ended March 31, 2022\n",
            "\n",
            "Commission File Numb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llama_parse_documents[1].text[:300])"
      ],
      "metadata": {
        "id": "MmDmobyTnDO1",
        "outputId": "a31ffd9e-0431-4376-fbc5-6acfc3bf36a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Uber Technologies, Inc. Quarterly Report\n",
            "\n",
            "# UBER TECHNOLOGIES, INC. - QUARTERLY REPORT\n",
            "\n",
            "# Table of Contents\n",
            "\n",
            "- Special Note Regarding Forward-Looking Statements\n",
            "- PART I - FINANCIAL INFORMATION\n",
            "- - Item 1. Financial Statements (unaudited)\n",
            "- Item 2. Management's Discussion and Analysis of Financial\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk.download('punkt_tab')  # Remove this line\n",
        "\n",
        "# Download the resource manually\n",
        "!python -m nltk.downloader punkt_tab\n",
        "!python -m nltk.downloader averaged_perceptron_tagger_eng"
      ],
      "metadata": {
        "id": "9OO0XocOpOv_",
        "outputId": "1e62652c-8bfd-4e60-8a99-3bdad91f96ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loader = DirectoryLoader('data/', glob=\"**/*.md\", show_progress=True)\n",
        "documents = loader.load()\n",
        "# Split loaded documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "#len(docs)\n",
        "print(f\"length of documents loaded: {len(documents)}\")\n",
        "print(f\"total number of document chunks generated :{len(docs)}\")\n",
        "#docs[0]\n",
        "\n",
        "# Initialize Embeddings\n",
        "embed_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "# Create and persist a Chroma vector database from the chunked documents\n",
        "vs = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embed_model,\n",
        "    persist_directory=\"chroma_db_llamaparse1\",  # Local mode with in-memory storage only\n",
        "    collection_name=\"rag\"\n",
        ")\n",
        "\n",
        "#query it\n",
        "#query = \"what is the agend of Financial Statements for 2022 ?\"\n",
        "#found_doc = qdrant.similarity_search(query, k=3)\n",
        "#print(found_doc[0][:100])\n",
        "#print(qdrant.get())\n",
        "\n",
        "print('Vector DB created successfully !')"
      ],
      "metadata": {
        "id": "OdHYqhybj3-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vector database\n",
        "def create_vector_database():\n",
        "    \"\"\"\n",
        "    Creates a vector database using document loaders and embeddings.\n",
        "\n",
        "    This function loads urls,\n",
        "    splits the loaded documents into chunks, transforms them into embeddings using OllamaEmbeddings,\n",
        "    and finally persists the embeddings into a Chroma vector database.\n",
        "\n",
        "    \"\"\"\n",
        "    # Call the function to either load or parse the data\n",
        "    llama_parse_documents = load_or_parse_data()\n",
        "    print(llama_parse_documents[0].text[:300])\n",
        "\n",
        "    with open('data/output.md', 'a') as f:  # Open the file in append mode ('a')\n",
        "        for doc in llama_parse_documents:\n",
        "            f.write(doc.text + '\\n')\n",
        "\n",
        "    markdown_path = \"/content/data/output.md\"\n",
        "    loader = UnstructuredMarkdownLoader(markdown_path)\n",
        "\n",
        "   #loader = DirectoryLoader('data/', glob=\"**/*.md\", show_progress=True)\n",
        "    documents = loader.load()\n",
        "    # Split loaded documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    #len(docs)\n",
        "    print(f\"length of documents loaded: {len(documents)}\")\n",
        "    print(f\"total number of document chunks generated :{len(docs)}\")\n",
        "    #docs[0]\n",
        "\n",
        "    # Initialize Embeddings\n",
        "    embed_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "    # Create and persist a Chroma vector database from the chunked documents\n",
        "    vs = Chroma.from_documents(\n",
        "        documents=docs,\n",
        "        embedding=embed_model,\n",
        "        persist_directory=\"chroma_db_llamaparse1\",  # Local mode with in-memory storage only\n",
        "        collection_name=\"rag\"\n",
        "    )\n",
        "\n",
        "    #query it\n",
        "    #query = \"what is the agend of Financial Statements for 2022 ?\"\n",
        "    #found_doc = qdrant.similarity_search(query, k=3)\n",
        "    #print(found_doc[0][:100])\n",
        "    #print(qdrant.get())\n",
        "\n",
        "    print('Vector DB created successfully !')\n",
        "    return vs,embed_model"
      ],
      "metadata": {
        "id": "TjggfkDHe-wn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vs,embed_model = create_vector_database()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "PC2WjlV7fDR2",
        "outputId": "aeabb367-0ad1-4e0c-a872-eabfb0dca5ec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-71cde060ef48>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membed_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vector_database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-477edc11772f>\u001b[0m in \u001b[0;36mcreate_vector_database\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Call the function to either load or parse the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mllama_parse_documents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_or_parse_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllama_parse_documents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/output.md'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Open the file in append mode ('a')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model = ChatGroq(temperature=0,\n",
        "                      model_name=\"mixtral-8x7b-32768\",\n",
        "                      api_key=groq_api_key,)"
      ],
      "metadata": {
        "id": "T4jC6HUxfRzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " vectorstore = Chroma(embedding_function=embed_model,\n",
        "                      persist_directory=\"chroma_db_llamaparse1\",\n",
        "                      collection_name=\"rag\")\n",
        " #\n",
        " retriever=vectorstore.as_retriever(search_kwargs={'k': 3})"
      ],
      "metadata": {
        "id": "yxiKaOvGfd9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Only return the helpful answer below and nothing else.\n",
        "Helpful answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4CiojQoZfex8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_custom_prompt():\n",
        "    \"\"\"\n",
        "    Prompt template for QA retrieval for each vectorstore\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(template=custom_prompt_template,\n",
        "                            input_variables=['context', 'question'])\n",
        "    return prompt\n",
        "#\n",
        "prompt = set_custom_prompt()\n",
        "prompt\n",
        "\n",
        "########################### RESPONSE ###########################\n",
        "PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of information to answer the user's question.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nContext: {context}\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\")"
      ],
      "metadata": {
        "id": "LbExOngIfifc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=chat_model,\n",
        "                               chain_type=\"stuff\",\n",
        "                               retriever=retriever,\n",
        "                               return_source_documents=True,\n",
        "                               chain_type_kwargs={\"prompt\": prompt})"
      ],
      "metadata": {
        "id": "Am7rHqgmfpnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = qa.invoke({\"query\": \"what is the Balance of UBER TECHNOLOGIES, INC.as of December 31, 2021?\"})"
      ],
      "metadata": {
        "id": "DK0WVcLlfv3k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}